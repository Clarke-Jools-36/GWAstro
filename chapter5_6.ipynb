{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4VQlGh-erzD"
      },
      "source": [
        "# Chapter 5 - Optimal detection statistic\n",
        "\n",
        "This notebook is based in part on https://github.com/losc-tutorial/Data_Guide, and introduces the match filter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57cs5KouerzI"
      },
      "source": [
        "## Imports\n",
        "\n",
        "We will need some standard imports for this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q6i5agZ-erzI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5MBs40uBerzK"
      },
      "outputs": [],
      "source": [
        "from scipy.signal import get_window"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IMKGlRCerzL"
      },
      "source": [
        "And we will need the GW-specific software `gwpy` and `pycbc`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bqeieLE2erzL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9aee24f-2845-4171-a2c1-ee4fb9f2e425"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 KB\u001b[0m \u001b[31m773.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ligo-segments (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install -q gwpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jrdNm-D5erzM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a97ac3d7-1f57-450e-a3c9-3d20502bf3e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 KB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.2/201.2 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.7/299.7 KB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lscsoft-glue (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pegasus-wms.api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-ligo-lw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pegasus-wms.common (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install -q lalsuite pycbc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "R4tn4LWmerzN"
      },
      "outputs": [],
      "source": [
        "from gwpy.timeseries import TimeSeries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U1KeG_DerzN"
      },
      "source": [
        "## Get the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Htdoebd9erzO"
      },
      "outputs": [],
      "source": [
        "time_center = 1126259462"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAskIgtwerzQ"
      },
      "outputs": [],
      "source": [
        "# This might be the slowest cell execution, as it downloads the data. Run it only once !\n",
        "strain_H1 = TimeSeries.fetch_open_data('H1', time_center - 512, time_center + 512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msxEGAj8erzQ"
      },
      "source": [
        "## Fourier transforms\n",
        "\n",
        "### Normalisations\n",
        "\n",
        "We have seen so far three implementation of FFTs, all with different normalisations:\n",
        "- numpy FFT, which implements the DFT without any normalisation\n",
        "- gwpy FFT, which applies the 1/N normalisation, thereby providing the actual DFT values\n",
        "- pycbc FFT, which applies a delta_t normalisation, thereby being our standard Fourier Transform (equations 0.1 of the textbook) in the continuous limit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DlBIPsoerzR"
      },
      "outputs": [],
      "source": [
        "# Setting up a test Time Series:\n",
        "timeseries = TimeSeries([1.0, 0.0, -1.0, 0.0], sample_rate=.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lwv0-2TNerzS"
      },
      "outputs": [],
      "source": [
        "# Numpy FFT:\n",
        "np.fft.rfft(timeseries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOl06FNPerzU"
      },
      "outputs": [],
      "source": [
        "# GWpy FFT:\n",
        "timeseries.fft()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B26o0EdNerzV"
      },
      "outputs": [],
      "source": [
        "# pyCBC FFT:\n",
        "ts_pycbc=timeseries.to_pycbc()\n",
        "print(ts_pycbc.to_frequencyseries())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyLdBBaEerzV"
      },
      "source": [
        "All have their advantages: `numpy`'s convention is simplest, and lets the user use that FFT with the normalisation they wish (remember, the FFT can be used for many computations, e.g. correlations). `GWpy`'s is the standard DFT, and does not change value for longer time series, representing instead the underlying frequency features independently of the lenght of time they are recorded. It is in general the best one for engineering applications, such as detector commissioning or characterisation. `pyCBC`'s is the one that relates most straightforwardly to the standard continuous Fourier transform, and is in general the better one for signal analysis and computing overlaps, matches and SNR.\n",
        "\n",
        "All those conventions are self-consistent, and it is always possible to go from one to the other. It's important to keep track, and it can be useful to choose the convention where the calculations one wants to do are simplest:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p81NUmC2erzW"
      },
      "outputs": [],
      "source": [
        "# Numpy FFT:\n",
        "np.fft.rfft(timeseries)*timeseries.dt.value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqwpIgxserzW"
      },
      "outputs": [],
      "source": [
        "# GWpy FFT:\n",
        "timeseries.fft()*timeseries.duration.value/2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbZmntAYerzX"
      },
      "source": [
        "### Windows\n",
        "\n",
        "Remember how we need to window the data to take FFTs without artifact? As we are reducing the data by some amount with the window, in practice we need to correct by a scaling factor of the window's mean:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7Qo_7mxerzX"
      },
      "outputs": [],
      "source": [
        "window=get_window(('tukey',1./4.),strain_H1.size)\n",
        "scaling_factor=np.abs(window).mean()\n",
        "\n",
        "strain_H1_fft=(strain_H1*window).fft()/scaling_factor\n",
        "strain_H1_afft=strain_H1.average_fft(window=('tukey',1./4.)) # built-in function which already includes the normalisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnvKlXwaerzX"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "f, (a0, a1) = plt.subplots(2, 1, sharex=True, gridspec_kw={'height_ratios': [3, 1]})\n",
        "\n",
        "a0.loglog(strain_H1_fft.frequencies,np.abs(strain_H1_fft),label='Built-in FFT')\n",
        "a0.loglog(strain_H1_afft.frequencies,np.abs(strain_H1_afft),label='Built-in FFT&window')\n",
        "a0.set_ylabel('Strain noise [$1/\\sqrt{\\mathrm{Hz}}$]')\n",
        "a0.legend(loc='lower left')\n",
        "\n",
        "a1.loglog(strain_H1_fft.frequencies,np.abs(strain_H1_fft-strain_H1_afft))\n",
        "a1.set_ylabel('Difference')\n",
        "\n",
        "a1.set_xlabel('Frequency [Hz]')\n",
        "a1.set_xlim(10,1024);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi1D1jfnerzY"
      },
      "source": [
        "We can now simply use the `average_fft()` method which includes the window scaling factor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E01e8z8BerzY"
      },
      "source": [
        "## Calculate the noise power spectral density (PSD)\n",
        "\n",
        "You have seen how to compute the amplitude spectrum manually last week. To get the Power Spectrum Density (PSD), from its definition (e.g. equation 7.6 in the textbook), we need a factor of $\\frac{2}{T}$:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq_sE5averzZ"
      },
      "outputs": [],
      "source": [
        "normalised_fft=strain_H1.average_fft(window=('tukey',1./4.))*(strain_H1.duration.value/2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FChXMrSzerzZ"
      },
      "outputs": [],
      "source": [
        "plt.loglog((2/strain_H1.duration.value)*np.abs(normalised_fft)**2,label='manual PSD')\n",
        "plt.loglog(strain_H1.psd(window=('tukey',1./4.)),label='Built-in PSD')\n",
        "plt.ylabel('Strain noise [$1/\\mathrm{Hz}$]')\n",
        "plt.xlabel('Frequency [Hz]')\n",
        "plt.xlim(10,1024)\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao4drtIPerzZ"
      },
      "source": [
        "Instead of keeping track of all the factors manually each time, we should write a function we can call. Or, better yet, use the ones already implemented _now that we know what they do_. Here we will use the `TimeSeries`'s `.psd()` method directly. Do look at the help message with\n",
        "\n",
        "```python\n",
        "help(strain_H1.psd)\n",
        "```\n",
        "\n",
        "And make note of any argument that might not be clear to discuss in class.\n",
        "\n",
        "To redude the variance in the PSD, we can average over several PSDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qg8MiPMerzZ"
      },
      "outputs": [],
      "source": [
        "Pxx_H1=strain_H1.psd(fftlength=4.,window=('tukey',1./4.),method='welch',overlap=2.)\n",
        "\n",
        "# A shorter section of the data, of the same lenght of the individual elements we are averaging over above:\n",
        "strain_H1_4s=strain_H1.crop(time_center - 2,time_center + 2)\n",
        "\n",
        "tukey_Pxx_H1=strain_H1_4s.psd(fftlength=4.,window=('tukey',1./4.))\n",
        "nowin_Pxx_H1=strain_H1_4s.psd(fftlength=4.,window='boxcar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_zOd5lwerza"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "# scale x and y axes\n",
        "plt.xscale('log', base=2)\n",
        "plt.yscale('log', base=10)\n",
        "\n",
        "# plot nowindow, tukey, welch together \n",
        "plt.plot(nowin_Pxx_H1.frequencies, nowin_Pxx_H1,'purple',label= 'No Window',\n",
        "         alpha=.8, linewidth=.5)\n",
        "plt.plot(tukey_Pxx_H1.frequencies, tukey_Pxx_H1 ,'green',label='Tukey Window',\n",
        "         alpha=.8, linewidth=.5)\n",
        "plt.plot(Pxx_H1.frequencies, Pxx_H1,'black',label='Welch Average', alpha=.8,\n",
        "         linewidth=.5)\n",
        "\n",
        "# plot 1/f^2\n",
        "# give it the right starting scale to fit with the rest of the plots\n",
        "# don't include zero frequency\n",
        "scale=nowin_Pxx_H1.value_at(32)*(32**2)\n",
        "plt.plot(nowin_Pxx_H1.frequencies[1:], scale/nowin_Pxx_H1.frequencies[1:]**2,'red',\n",
        "         label= r'$1 / f^2$', alpha=.8, linewidth=1)\n",
        "\n",
        "\n",
        "plt.axis([20, 512, 1e-48, 1e-41])\n",
        "plt.ylabel('Sn(f)')\n",
        "plt.xlabel('Freq (Hz)')\n",
        "plt.legend(loc='upper center')\n",
        "plt.title('LIGO PSD data near ' + str(time_center) + ' at H1')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLGQE2CUerza"
      },
      "source": [
        "# **Question**\n",
        "- Try to reproduce the [Welch Average](https://en.wikipedia.org/wiki/Welch%27s_method) PSD manually, by computing PSDs in chunks of time of 4 seconds long, overlapping by 2 seconds (e.g., every point in the time series gets included in the calculation of 2 PSDs), and take the average."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIh1-fPHerza"
      },
      "source": [
        "Note that inadequate windowing of these strongly colored data produces a psd that \n",
        "is entirely dominated by \"spectral leakage\",\n",
        "https://en.wikipedia.org/wiki/Spectral_leakage,\n",
        "and inadequate averaging leads to noise fluctuations that contaminate the estimated PSD.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVwGKcKYerza"
      },
      "source": [
        "## Plot strain data that has been windowed, bandpassed, and whitened\n",
        "\n",
        "In the chapter 4 notebook you applied _filters_ to the data: transforming a time series in the frequency domain, multiplying the frequency series by a function to remove or downweigh some frequency, and converting the result back into the time domain.\n",
        "\n",
        "We can use the amplitude spectral density for this operation, i.e. whitening the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pE-HMUJerzb"
      },
      "outputs": [],
      "source": [
        "# plot original strain data\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(4, 1, 1)\n",
        "\n",
        "plt.plot(strain_H1_4s.times, strain_H1_4s, 'blue', \n",
        "         label='Hanford Data', linewidth=.5)\n",
        "plt.legend()\n",
        "\n",
        "# plot windowed data\n",
        "plt.subplot(4, 1, 2)\n",
        "strain_H1_4s_win=strain_H1_4s*get_window(('tukey',1./4.),strain_H1_4s.size)\n",
        "plt.plot(strain_H1_4s_win.times, strain_H1_4s_win, 'green', \n",
        "         label='Windowed Data', linewidth=.5)\n",
        "plt.legend()\n",
        "\n",
        "# plot whitened data\n",
        "plt.subplot(4, 1, 3)\n",
        "\n",
        "# For demonstration purposes, we are computing the whitened data in 3 ways.\n",
        "# First, using the built-in \"whiten\" function, using it to compute the PSD on\n",
        "# the whole data (identically to the calculations in the previous section)\n",
        "# and only keeping the 4 seconds of interest.\n",
        "strain_H1_white = strain_H1.whiten(fftlength=4,overlap=2,window=('tukey',1./4.))\n",
        "strain_white=strain_H1_white.crop(time_center - 2,time_center + 2)\n",
        "\n",
        "# We can also use the \"whiten\" function directly on the 4-second long data segment,\n",
        "# however, as it does not have enough data for an accurate PSD estimation with averages,\n",
        "# we pass directly the value of the ASD.\n",
        "strain_white_v2 = strain_H1_4s_win.whiten(asd=np.sqrt(Pxx_H1),window=('tukey',1./4.))\n",
        "\n",
        "# Finally, we can also do it all manually, dividing by the ASD=PSD**0.5, and taking care\n",
        "# of the normalisation.\n",
        "strain_white_v3 =(strain_H1_4s.average_fft(window=('tukey',1./4.))/(Pxx_H1**(1/2))).ifft()*(1./np.sqrt(1./(strain_H1_4s.dt*2)))\n",
        "\n",
        "plt.plot(strain_white.times, strain_white, 'red', \n",
        "         label='Whitened Data (built-in)', linewidth=.5)\n",
        "plt.plot(strain_white_v2.times, strain_white_v2, \n",
        "         label='Whitened Data (built-in with PSD)', linewidth=.5)\n",
        "plt.plot(strain_white_v3.times, strain_white_v3, \n",
        "         label='Whitened Data (manual)', linewidth=.5)\n",
        "plt.legend()\n",
        "\n",
        "# plot bandpassed data\n",
        "plt.subplot(4, 1, 4)\n",
        "strain_bp = strain_white.bandpass(35.,350.)\n",
        "plt.plot(strain_bp.times, strain_bp, 'black', \n",
        "         label='Bandpassed Data', linewidth=.5)\n",
        "\n",
        "plt.legend()\n",
        "#plt.yticks([-6, -3, 0, 3, 6, 9])\n",
        "#plt.tight_layout()\n",
        "#plt.xlim([-2, 2])\n",
        "#plt.ylim([-8, 8])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtYfrof5erzb"
      },
      "source": [
        "## Fourier phases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCXH3qfaerzb"
      },
      "outputs": [],
      "source": [
        "# Plots Fourier phases of strain data both with and without a spectral window\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "strain_H1_fft = strain_H1_4s.average_fft()\n",
        "strain_H1_fft_win = strain_H1_4s.average_fft(window=('tukey',1./4.))\n",
        "\n",
        "plt.plot(strain_H1_fft.frequencies, np.angle(strain_H1_fft), '.', label='No window', color='blue', markersize=1)\n",
        "plt.plot(strain_H1_fft_win.frequencies, np.angle(strain_H1_fft_win), '.', label='Tukey window', color='red', markersize=1)\n",
        "\n",
        "plt.xlim([30, 400])\n",
        "plt.xlabel('f(Hz)')\n",
        "plt.ylabel('Phase')\n",
        "plt.legend(loc='upper right', fontsize=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiX-MAqKerzc"
      },
      "source": [
        "Note that the fourier phases for properly windowed data (random noise) are random between [0, 2*pi];\n",
        "by contrast, un-windowed data produce strongly correlated fourier phases that are artifacts of the abrupt beginning and end of the data stretch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WisTS1_Derzc"
      },
      "source": [
        "## Generate, taper, and zero-pad the waveform template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPnq5y9yerzc"
      },
      "outputs": [],
      "source": [
        "# Importing the waveform generators\n",
        "from pycbc.waveform import get_td_waveform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsqPwdFSerzc"
      },
      "source": [
        "We will generate a gravitational waveform below, taking values from [this paper](https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.116.241102). However, do note the following caveats that apply, and which we will cover later in class:\n",
        "* we are ignoring correlations between parameters.\n",
        "* we are ignoring network effects, and only use the \"plus\" polarisation of the signal\n",
        "* we are taking mean 1D-posterior density values, ignoring prior and density effects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5xMcnxRerzc"
      },
      "outputs": [],
      "source": [
        "m1 = 38.9 # Solar masses\n",
        "m2 = 31.6 # Solar masses\n",
        "\n",
        "h_plus, _ = get_td_waveform(approximant=\"SEOBNRv4_opt\",\n",
        "                     mass1=m1,\n",
        "                     mass2=m2,\n",
        "                     delta_t=strain_H1.dt.value,\n",
        "                     f_lower=20)\n",
        "\n",
        "plt.plot(h_plus.sample_times, h_plus,label='Before Taper')\n",
        "plt.plot(h_plus.sample_times, h_plus*get_window(('tukey',1/4),h_plus.shape[0]),label='After Taper')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JEhHHFHerzd"
      },
      "source": [
        "The effect of the tapering Tukey window is visible at the beginning of the template.  Tapering forces the template to have zero values at the ends, in order to avoid artefacts due to discontinuities when taking an FFT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ5c-CeLerzd"
      },
      "source": [
        "# Chapter 6 - Matched filter\n",
        "\n",
        "## Calculate the matched filter result for GW150914\n",
        "\n",
        "Note, the results of matched filtering are sensitive to small changes in PSD estimation, and so running similar codes may not give identical results. The calculated SNR is particularly sensitive, and differences of a few percent between different implementations are typically expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gwzMkPmerzd"
      },
      "outputs": [],
      "source": [
        "strain_H1_white = strain_H1.whiten(fftlength=4,overlap=2,window=('tukey',1./4.))\n",
        "# Getting a more manageable 32 seconds of data:\n",
        "strain_white=strain_H1_white.crop(time_center - 16,time_center + 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NI4iJy3erzd"
      },
      "outputs": [],
      "source": [
        "# Badpassing the whitened strain with specific values are defined in the detection paper\n",
        "strain_bp=strain_white.bandpass(35.,350.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKmHQMjKerze"
      },
      "outputs": [],
      "source": [
        "plt.plot(strain_bp)\n",
        "plt.xlim(time_center+0.25,time_center+0.46)\n",
        "plt.ylabel('H1 strain')\n",
        "plt.xlabel('time [s]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRGuf9cqerze"
      },
      "source": [
        "As we have see in previous notebooks, this is were the signal is. Now we will set-up the template to match-filter the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwkal0Tderze"
      },
      "outputs": [],
      "source": [
        "# Making it 32 seconds:\n",
        "h_plus.prepend_zeros(np.ceil((16+h_plus.start_time)/h_plus.delta_t))\n",
        "h_plus.append_zeros(np.floor((16-h_plus.end_time)/h_plus.delta_t))\n",
        "\n",
        "template=TimeSeries.from_pycbc(h_plus)\n",
        "template.duration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q3I2euIerze"
      },
      "outputs": [],
      "source": [
        "# Same with the data:\n",
        "strain_H1_32=strain_H1.crop(time_center - 16,time_center + 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RA7kXW6Cerze"
      },
      "outputs": [],
      "source": [
        "# FFT of the data, with the appropriate normalisation\n",
        "data_f=strain_H1_32.average_fft(window=('tukey',1./4.))*(strain_H1_32.duration/2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNTPhCB1erzf"
      },
      "outputs": [],
      "source": [
        "# FFT of the template, with the appropriate normalisation\n",
        "template_f=template.average_fft(window=('tukey',1./4.))*(template.duration/2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wT3vfrnerzf"
      },
      "outputs": [],
      "source": [
        "# We will need the PSD with the same frequency spacing as the data and template,\n",
        "# so we interpolate it to match:\n",
        "Pxx_H1_32=Pxx_H1.interpolate(data_f.df.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kKJpo57erzf"
      },
      "outputs": [],
      "source": [
        "# With the right normalisation, this is equation 7.58 of the textbook:\n",
        "optimal=data_f*template_f.conjugate()/Pxx_H1_32\n",
        "opt_time=2*optimal.ifft()*(optimal.df*2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vq1GnizEerzf"
      },
      "outputs": [],
      "source": [
        "# This is equation 7.49 of the textbook: the overlap of the template with itself\n",
        "sigmasq = 4 * np.real((template_f * template_f.conjugate() / Pxx_H1_32).sum() * template_f.df)\n",
        "sigma = np.sqrt(np.abs(sigmasq))\n",
        "\n",
        "# And now we have the SNR time series:\n",
        "SNR_complex = opt_time/sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4U_iLLKerzf"
      },
      "outputs": [],
      "source": [
        "# We can recenter thing with the location of peak in the template:\n",
        "peaksample = template.argmax()  \n",
        "SNR_complex = np.roll(SNR_complex,peaksample)\n",
        "SNR = abs(SNR_complex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JsxCmc0erzg"
      },
      "outputs": [],
      "source": [
        "SNRmax=SNR.max().value\n",
        "time_max=SNR.times[SNR.argmax()]\n",
        "print('Maximum SNR of {} at {}.'.format(SNRmax,time_max))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObmsOa1aerzg"
      },
      "outputs": [],
      "source": [
        "plt.plot(SNR.times,SNR)\n",
        "plt.xlabel('time [s]')\n",
        "plt.ylabel('SNR');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkF7AE7Rerzg"
      },
      "outputs": [],
      "source": [
        "plt.plot(SNR.times,SNR)\n",
        "plt.xlim(time_center+0.35,time_center+0.5)\n",
        "plt.xlabel('time [s]')\n",
        "plt.ylabel('SNR');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCEESNXwerzh"
      },
      "source": [
        "# **Questions**\n",
        "\n",
        "- Compare to how much better this is than the correlation in chapter3\n",
        "- why is it not exactly at `time_center` ?\n",
        "- do this for `L1` !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9UkT4ajerzh"
      },
      "source": [
        "We can find the amplitude and phase that maximise the likelihood to plot the template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsyPo4Yxerzh"
      },
      "outputs": [],
      "source": [
        "# This is equation 7.47 of the textbook\n",
        "a_max=SNRmax/sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UjaCdMQerzh"
      },
      "outputs": [],
      "source": [
        "# Getting the optimal phase from the complex SNR:\n",
        "phase = -np.angle(SNR_complex[SNR.argmax()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9mmhSRterzh"
      },
      "outputs": [],
      "source": [
        "# adjust the amplitude, phase and time of the template:\n",
        "template_transformed=(template.fft()*np.exp(- 1.j * phase.value)).ifft()*a_max\n",
        "template_transformed.x0=template.x0+time_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-LbH3Yjerzi"
      },
      "outputs": [],
      "source": [
        "# whitening and bandpassing the template in the same way we did the data for comparison:\n",
        "template_whitened_bandpassed=template_transformed.whiten(asd=np.sqrt(Pxx_H1),window=('tukey',1./4.)).bandpass(35.,350.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfyNcZAFerzi"
      },
      "outputs": [],
      "source": [
        "plt.plot(strain_bp)\n",
        "plt.plot(template_whitened_bandpassed)\n",
        "plt.xlim(time_center+0.25,time_center+0.46)\n",
        "plt.ylabel('H1 strain')\n",
        "plt.xlabel('time [s]');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuJRv2ylerzi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3F8Gt62erzi"
      },
      "source": [
        "## Calculating the background and significance\n",
        "\n",
        "This is an illustrative example of significance measurement, using p-values. In practice we can use multiple detectors to get much better statistics. \n",
        "\n",
        "We calculate the peak in the SNR for a given window around the observed SNR peak, this is our \"on-source\" value. To determine the significance of the on-source we will compare how likely it is for a peak as large or larger to appear in the background. Our background will be empirically measured by taking portions of the SNR time series from the \"off-source\" i.e. times that do not overlap the on-source. An important criteria to avoid a biased significance estimate is that the background and experiment be performed in the same manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewoxnbA0erzi"
      },
      "outputs": [],
      "source": [
        "# convert the times to indices along with how large the region is in number of samples:\n",
        "window_size = int((1) / SNR.dt.value)\n",
        "sidx = int((float(-0.25+time_center-SNR.x0.value)) / SNR.dt.value)\n",
        "eidx = sidx + window_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nMmohOierzi"
      },
      "source": [
        "For this illustrative example, our choice of window is arbitrary, and we check that the \"on-source\" does contain the peak SNR:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gCnzn_kerzj"
      },
      "outputs": [],
      "source": [
        "plt.plot(SNR,label='SNR time series')\n",
        "plt.axvline(SNR.times[sidx].value,c='r',label='window')\n",
        "plt.axvline(SNR.times[eidx].value,c='r')\n",
        "plt.xlabel('time [s]')\n",
        "plt.ylabel('SNR')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXPXLqjkerzj"
      },
      "outputs": [],
      "source": [
        "# Calculate the \"on-source\" peak SNR statistic value.\n",
        "onsource = SNR[sidx:eidx].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4m2urG83erzj"
      },
      "outputs": [],
      "source": [
        "# Now that we've calculated the onsource peak, we should calculate the background peak values.\n",
        "# We do this by chopping up the time series into chunks that are the same size as our\n",
        "# onsource and repeating the same peak finding (max) procedure.\n",
        "\n",
        "# Walk through the data in chunks and calculate the peak statistic value in each.\n",
        "peaks = []\n",
        "i = 0\n",
        "while i + window_size < len(SNR):\n",
        "    p = SNR[i:i+window_size].max()\n",
        "    peaks.append(p)\n",
        "    i += window_size\n",
        "    \n",
        "    # Skip past the onsource time\n",
        "    if abs(i - sidx) < window_size:\n",
        "        i += window_size * 2\n",
        "    \n",
        "peaks = np.array([p.value for p in peaks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvJpRAjjerzj"
      },
      "outputs": [],
      "source": [
        "# The p-value is just the number of samples observed in the background with a \n",
        "# value equal or higher than the onsource divided by the number of samples.\n",
        "# We can make the mapping between statistic value and p-value using our background\n",
        "# samples.\n",
        "pcurve = np.arange(1, len(peaks)+1)[::-1] / float(len(peaks))\n",
        "peaks.sort()\n",
        "\n",
        "pvalue = (peaks > onsource.value).sum() / float(len(peaks))\n",
        "\n",
        "plt.figure(figsize=[10, 7])\n",
        "plt.scatter(peaks, pcurve, label='Off-source (Noise Background)', color='black')\n",
        "\n",
        "plt.axvline(onsource.value, label='On-source', color='red')\n",
        "plt.axhline(pvalue, color='red')\n",
        "\n",
        "plt.legend()\n",
        "plt.yscale('log')\n",
        "plt.grid()\n",
        "plt.ylim(1e-2, 1e0)\n",
        "plt.ylabel('p-value')\n",
        "plt.xlabel('SNR')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"The p-value associated with the GW150914 peak is {}\".format(pvalue))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYSo-_XZerzj"
      },
      "source": [
        "# **Questions**\n",
        "\n",
        "- Why is the p-value associated with the peak zero? What does that mean?\n",
        "- How can we use multiple detector's data to improve our significance measurement?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2L6wSpberzk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "chapter5_6.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}